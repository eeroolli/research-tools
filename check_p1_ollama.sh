#!/bin/bash
# Commands to run on p1 to diagnose Ollama setup

echo "=== Commands to run on p1 (SSH to p1 first) ==="
echo ""
echo "1. Check all Ollama processes:"
echo "   ps aux | grep ollama | grep -v grep"
echo ""
echo "2. Check Docker containers:"
echo "   docker ps | grep ollama"
echo ""
echo "3. Check if Ollama is running as system service:"
echo "   systemctl status ollama 2>/dev/null || echo 'Not a system service'"
echo ""
echo "4. Check models in Docker container (if exists):"
echo "   docker exec ollama-gpu ollama list"
echo ""
echo "5. Check models in system Ollama (if running outside Docker):"
echo "   curl http://localhost:11434/api/tags | python3 -m json.tool"
echo ""
echo "6. Check Docker volumes (where models might be stored):"
echo "   docker volume ls | grep ollama"
echo "   docker volume inspect <ollama-volume-name>  # if found"
echo ""
echo "7. Check WebUI configuration (which Ollama it connects to):"
echo "   # Check WebUI container environment variables"
echo "   docker ps --format 'table {{.Names}}\t{{.Image}}' | grep -i webui"
echo "   docker inspect <webui-container> | grep -i OLLAMA"
echo ""
echo "8. Check which port Ollama is listening on:"
echo "   netstat -tlnp | grep 11434"
echo "   # or"
echo "   ss -tlnp | grep 11434"
echo ""
echo "=== Most likely scenario ==="
echo "The WebUI is connecting to a system Ollama service (not Docker),"
echo "which has the models. The Docker container (ollama-gpu) might be"
echo "a separate instance without models, or the models are in a shared volume."

