# Data Structure Cleanup Plan

## Current Problem
The project has scattered data directories and log files across multiple locations, creating confusion and maintenance issues.

## Current Data Structure Issues

### **Scattered Data Directories:**
```
./data/                          # âœ… Main data directory (correct)
â”œâ”€â”€ books/                       # âœ… For book processing results
â”œâ”€â”€ papers/                      # âœ… For paper processing results  
â”œâ”€â”€ logs/                        # âœ… For application logs
â”œâ”€â”€ cache/                       # âœ… For temporary cache
â”œâ”€â”€ output/                      # âœ… For processed outputs
â””â”€â”€ temp/                        # âœ… For temporary files

./process_books/data/            # âŒ Duplicate - should be removed
â””â”€â”€ logs/                        # âŒ Duplicate logs

./process_papers/data/           # âŒ Empty - should be removed

./scripts/data/                  # âŒ Duplicate - should be removed
â””â”€â”€ logs/                        # âŒ Duplicate logs

./shared_tools/metadata/         # âœ… Code directory - keep as is
â””â”€â”€ extractor.py                 # âœ… Code file - keep as is
```

### **Scattered Log Files:**
```
./data/logs/processing_20251003_122311.log     # âœ… Main logs
./data/logs/processing_20251003_132703.log     # âœ… Main logs

./process_books/data/logs/processing_20251003_122311.log  # âŒ Duplicate
./scripts/data/logs/processing_20251003_124507.log        # âŒ Duplicate
./scripts/data/logs/processing_20251003_125842.log        # âŒ Duplicate
./scripts/data/logs/processing_20251003_134853.log        # âŒ Duplicate
./scripts/data/logs/processing_20251003_140328.log        # âŒ Duplicate
./scripts/data/logs/processing_20251003_141215.log        # âŒ Duplicate
./scripts/data/logs/processing_20251003_143120.log        # âŒ Duplicate
```

## Cleanup Plan

### **Phase 1: Consolidate Logs** ğŸ§¹
1. **Move all logs to main data/logs directory**
   - Copy unique logs from scattered locations
   - Remove duplicate log files
   - Update logging configuration to use only `data/logs/`

2. **Update logging configuration**
   - Ensure all scripts write to `data/logs/`
   - Remove hardcoded paths to old log locations
   - Standardize log file naming

### **Phase 2: Remove Duplicate Data Directories** ğŸ—‘ï¸
1. **Remove `process_books/data/`**
   - Move any unique files to main `data/` directory
   - Remove empty directory

2. **Remove `process_papers/data/`**
   - Directory is empty, safe to remove

3. **Remove `scripts/data/`**
   - Move any unique files to main `data/` directory
   - Remove empty directory

### **Phase 3: Update Code References** ğŸ”§
1. **Update all hardcoded paths**
   - Search for references to old data directories
   - Update to use centralized `data/` directory
   - Ensure all scripts use relative paths from project root

2. **Update configuration files**
   - Update any config files that reference old paths
   - Ensure consistent path structure

### **Phase 4: Verify and Test** âœ…
1. **Test all scripts**
   - Ensure they can find data files in new locations
   - Verify logging works correctly
   - Check that no functionality is broken

2. **Document new structure**
   - Update README with clear data structure
   - Document where different types of files are stored

## Target Data Structure

### **Final Clean Structure:**
```
research-tools/
â”œâ”€â”€ data/                        # ğŸ¯ Single data directory
â”‚   â”œâ”€â”€ books/                   # Book processing results
â”‚   â”‚   â””â”€â”€ book_processing_log.csv
â”‚   â”œâ”€â”€ papers/                  # Paper processing results
â”‚   â”‚   â””â”€â”€ paper_processing_log.csv
â”‚   â”œâ”€â”€ logs/                    # All application logs
â”‚   â”‚   â”œâ”€â”€ processing_YYYYMMDD_HHMMSS.log
â”‚   â”‚   â””â”€â”€ error_YYYYMMDD_HHMMSS.log
â”‚   â”œâ”€â”€ cache/                   # Temporary cache files
â”‚   â”œâ”€â”€ output/                  # Processed outputs
â”‚   â””â”€â”€ temp/                    # Temporary files
â”œâ”€â”€ scripts/                     # All executable scripts
â”œâ”€â”€ process_books/               # Book processing code (no data/)
â”œâ”€â”€ process_papers/              # Paper processing code (no data/)
â””â”€â”€ shared_tools/                # Shared utilities
    â””â”€â”€ metadata/                # Code only
```

## Implementation Steps

### **Step 1: Backup Current State**
```bash
# Create backup before cleanup
cp -r data data_backup_$(date +%Y%m%d_%H%M%S)
```

### **Step 2: Consolidate Logs**
```bash
# Move all logs to main data/logs directory
find . -name "*.log" -not -path "./data/logs/*" -exec cp {} data/logs/ \;
find . -name "*.log" -not -path "./data/logs/*" -delete
```

### **Step 3: Remove Duplicate Directories**
```bash
# Remove empty duplicate data directories
rm -rf process_books/data
rm -rf process_papers/data  
rm -rf scripts/data
```

### **Step 4: Update Code References**
- Search for hardcoded paths to old data directories
- Update all references to use centralized `data/` directory
- Update logging configuration

### **Step 5: Test and Verify**
- Run all scripts to ensure they work
- Verify logs are written to correct location
- Check that data files are found correctly

## Benefits of Clean Structure

1. **Single Source of Truth**: All data in one place
2. **Easier Maintenance**: No confusion about where files are
3. **Better Organization**: Clear separation of concerns
4. **Simpler Backups**: Only need to backup `data/` directory
5. **Cleaner Code**: No hardcoded paths to multiple locations
6. **Better Documentation**: Clear structure is easier to document

## Files to Update

### **Code Files with Hardcoded Paths:**
- `scripts/find_isbn_from_photos.py` - Update log path
- `process_books/src/utils/file_manager.py` - Update log path
- Any other scripts that reference old data directories

### **Configuration Files:**
- `process_books/config/process_books.conf` - Update paths
- Any other config files with data directory references

## Risk Mitigation

1. **Backup First**: Always backup before making changes
2. **Test Incrementally**: Test after each step
3. **Update Code First**: Update code references before removing directories
4. **Verify Functionality**: Ensure all scripts still work after cleanup

---

**Priority**: High - This cleanup will significantly improve project maintainability and reduce confusion.
